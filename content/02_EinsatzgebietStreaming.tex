\section{Einsatzgebiet Streamverarbeitung}
Bei der Streamverarbeitung treten einige Anforderungen auf, 
die es bei der Bearbeitung der anfallenden Daten zu berücksichtigen gilt. 
So treffen die Daten in einem Stream beispielsweise kontinuierlich ein 
und sollten abgearbeitet werden, bevor die nächsten Daten eintreffen. 
Außerdem sollen die aus dem Stream gewonnenen Informationen möglichst in Echtzeit abrufbar bleiben.
Auch die Datenmenge in einem Stream kann stark variieren, wodurch ein hohes Maß an Skalierbarkeit gefordert werden kann.

Unter diesen Anforderungen bieten sich approximative Algorithmen an, denn sie bieten eine schnellere Ausführungszeit und eine bessere Skalierbarkeit als exakte Algorithmen. 
Für diesen Vorteil büßen sie jedoch ihre absolute Genauigkeit ein. 
In der Praxis überwiegt der Gewinn von Geschwindigkeit dem Verlust an Genauigkeit jedoch häufig \cite{maas2019}. 

Die Datenverarbeitung in einem Stream durch approximative Algorithmen unterliegt den folgenden drei Eigenschaften \cite{maas2019}:  


\begin{itemize}
\item
Der Genauigkeit der erzeugten Ergebnisse,
\item
der Ausführung in Echtzeit (Performanz)
\item
und der zu verarbeitende Datenmenge
\end{itemize}

Diese beeinflussen sich gegenseitig was dazu führt, 
dass es fast unmöglich ist für alle drei Eigenschaften die bestmöglichen Resultate zu erzielen.
So nimmt beispielsweise die Präzision approximativer Algorithmen ab, 
wenn sie auf die Verwendung für große Datenmengen oder eine möglichst hohe Performanz abzielen \cite{maas2019}.

In Datenstreams kann es interessant sein zu wissen, 
wie die Verteilung der Daten ausfallen wird. 
So können beispielsweise Netzanbieter ermitteln, 
wie viele ihrer Kunden die Bandbreite erhalten die ihnen zur verfügung stehen sollte 
und können eine Störung erfassen, 
noch bevor sich größere Mengen von Kunden über eine zu niedrige Bandbreite beschweren können.
Auch kann es hilfreich sein, zu erfassen, 
wie viele verschiedenen Daten in einem Stream bereits aufgetreten sind. 
So lässt sich beispielsweise auf einer Webseite wie Wikipedia über einen Tag ermitteln wie viele verschiedene Nutzer Einträge verändert haben. 
Die ermittelte Zahl kann dann mit der Gesammtzahl an Änderungen über den Tag verglichen werden, 
um so zu erfahren, ob an einem Tag besonders viele Änderungen von besonders wenigen Nutzern gemacht wurden.
Für beide Anwendungsfälle eignet sich der Hyperloglog-Algorithmus besonders gut, 
weil er sich durch Schnelligkeit bei der Auswertung, 
Speicherplatzbedarf und Präzision bewiesen hat.