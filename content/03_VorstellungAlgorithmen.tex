\section{Vorstellung: Algorithmen}
In diesem Kapitel werden approximative Algorithmen vorgestellt, 
die in der Streamverarbeitung nutzbringend eingesetzt werden können.

\section{HyperLogLog}
\todo[inline]{erklärung hyperloglog}
hier wird der hyperloglog vorgestellt.
\subsection{Beispiele}
Der hyperloglog-Algorithmus wurde ursprünglich zur Aufspürung von Würmern und Computerviren entwickelt. 
Würmer und Viren öffnen für gewöhnlich eine große Anzahl an Verbindungen durch die sie versuchen einzudringen. 
Durch die überwachung des Datenverkehrs auf Basis von Kardinalitäten dieser Verbindungen konnten diese aufgespürt werden. \cite{flajolet2007}

\todo[inline]{vielleicht anpassen}

\subsection{T-Digest}
Der T-Digest ist ein von Ted Dunning entwickelter Algorithmus zum approximieren der Verteilung von Daten mit Quantilen.

Quantile sind Schwellenwerte auf einer Verteilungsfunktion, die für einen gegebenen Prozentsatz angeben, welche Werte unterhalb dieses Schwellenwerts liegen.
So liefert das 25\%-Quantil beispielweise den Wert, für den gilt, dass 25\% aller Werte in der zugrundeliegenden Datenmenge kleiner sind als dieser Wert.
Das 50\%-Quantil entspricht somit dem Median einer Datenmenge -- dem Wert, der genau in der Mitte der geordneten Liste steht.

Bei einer naiven Lösung dieser Aufgabe müssen alle eingehenden Daten in einer Liste gespeichert und geordnet werden, woraufhin anschließend die Quantilen ausgelesen werden können.
Beim T-Digest wird die Verteilung der Daten auf wichtige Punkte, in deren Umgebung viel Veränderung stattfindet, heruntergebrochen \cite{davidsonpilon2015}.
Diesen Schwerpunkten (engl. Centroids) wird ein Gewicht zugewiesen, woraus sich später eine gute Approximation der Verteilung ableiten lässt.
Laut Dunning \cite{dunning2015} ist hierbei eine höhere Präzision vor Allem in den Randbereichen der Verteilung von besonderer Wichtigkeit.
So sei eine Abweichung von etwa 0,5\% im Bereich des Medians in Ordnung, in Extremfällen wie dem 99,99\%-Quantil jedoch nicht mehr, da es hier darauf ankommt, einen sehr kleinen Bereich zu identifizieren.

In einem Test, in dem eine Implementierung des T-Digest-Algorithmus mit einer einfachen, sortierten Liste verglichen wurde, hat sich die Präzision des T-Digest als sehr gut herausgestellt.
Es wurden 100\,000 verschiedene, zufällige Zahlen von 1 bis 1\,000 in beide Objekte eingetragen, wobei anschließend je der Median, sowie das 99,99\%-Quantil geprüft wurden.

Hierbei ergab sich, dass der Median zwar nur in 0,33\% der Fälle akkurat war, jedoch nie eine Abweichung von $\pm$\,0,5\% überschritt.
Für das 99,99\%-Quantil war der T-Digest in 99,79\% der Fälle akkurat, in den übrigen Fällen wies er eine Abweichung von maximal $\pm$\,0,01\% auf.

Außerdem hat sich die von Dunning und Ertl \cite{dunning2019} beschriebene, sehr hohe Geschwindigkeit bei diesem Test ebenso bewahrheitet.
Sowohl das Einfügen in die Liste, als auch in den T-Digest bewegte sich durchschnittlich in der gleichen Größenordnung von 10\textsuperscript{-5}\,Sekunden (10\,\textmu{}s).
Gleiches gilt für das identifizieren eines Quantils, was in beiden Fällen in der Größenordnung 10\textsuperscript{-4}\,Sekunden (100\,\textmu{}s) passiert.

Ein weiterer Vorteil des T-Digest-Algorithmus ist, dass die Daten mehrerer Digests einfach verbunden werden können, solange man die Gewichte der Schwerpunkte beachtet \cite{dunning2019}.
Dies ist zum Beispiel nützlich, wenn man einen Median aus mehreren, großen Datensätzen identifizieren möchte, da es hierfür nicht einfach reicht, den Median für jeden Datensatz einzeln zu identifizieren und aus der Liste der Ergebnisse einen neuen Median zu ziehen.



\subsection{StreamApprox}
StreamApprox ist ein Sampling Algorithmus der dafür entwickelt wurde, 
die Datenmenge eines Streams auf eine bestimmte Größe zu limitieren, 
indem er nur einige der Daten des Streams zur weiterreicht. 
Dabei erfolgt die Selektierung welche der Daten weiter gegeben werden durch einen Zufallsalgorithmus. 
Auf diese Weise wird das Ergebnis der Auswertung des Streams weniger stark beeinträchtigt, 
als wenn die Daten, die die Verarbeitungskapazitäten überschreiten, 
überhaupt nicht betrachtet werden. 
Der Algorithmus ist dabei sowohl auf Batch-, als auch auf Pipeline-basierte Streams anwendbar.
Das besondere an StreamApprox ist die Fähigkeit, 
die Größe des Streams zu limitieren und dabei so wenig Daten wie möglich aus dem Stream zu entfernen. 
Der Algorithmus reduziert also nur dann den Datenstrom, wenn dieser ein eingestelltes Limit überschreitet. 
Diese Fähigkeit übersteigt die von random sampling Algorithmen, 
die für eine ähnliche Ausgabe die absolute Anzahl an Elementen des Streams kennen müssten. 
Ermöglicht wird das durch die Verwendung von reservoir Sampling, 
bei dem die Elemente des Streams in einen Pufferspeicher geschrieben werden, 
wobei neue Elemente an einer zufälligen Stelle des Puffers den vorherigen Wert überschreiben. \cite{quoc2017} 

Dieses Vorgehen kann hilfreich sein, 
wenn die Datenmenge für die gewünschte Analyse stark varriert oder unnötig groß ist und reduziert werden soll, 
um eine bestimmte Verarbeitungsgeschwindigkeit beizubehalten, 
ohne das Ergebnis der Analyse zu stark zu beeinflussen.
Es kann ebenfalls hilfreich sein, 
wenn, aufgrund von fehlender Ressourcen, zur Verarbeitung, die Menge an gelieferten Daten reduziert werden soll.

\todo[inline]{weitere algorithmen vorstellen...möglichweise aus jeder kategorie der Datasketches webseite einer}
