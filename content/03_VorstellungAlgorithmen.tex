\section{Vorstellung: Algorithmen}

Approximative Algorithmen dienen jeweils sehr spezifischen Aufgaben, die in einem großen Datensatz oder Stream gelöst werden können.
Da Probleme wie das Zählen unterschiedlicher Elemente oder das bestimmen eines häufigsten Elements mathematisch sehr unterschiedliche Herangehensweisen erfordern, können diese Algorithmen also nur für ein spezielles Aufgabengebiet eingesetzt werden.
Hier sollen einige Algorithmen mit unterschiedlichen Einsatzgebieten und deren Funktionsweisen grob vorgestellt werden, um einen Überblick über verschiedene Aufgabenbereiche und deren Lösungen zu bieten.

\subsection{HyperLogLog}

Der HyperLogLog-Algorithmus basiert auf dem Flajolet-Martin-Algorithmus, welcher erst von Durand und Flajolet \cite{durand2003} zum LogLog-Algorithmus und schließlich von Flajolet et al. \cite{flajolet2007} zum HyperLogLog-Algorithmus weiterentwickelt wurde.
Der Algorithmus dient der approximativen Lösung des sog. Count-Distinct-Problems, bei dem es die Anzahl unterschiedlicher, auftretender Elemente zu zählen gilt.
Da die naive Lösung dieses Problems, die Speicherung in einer Menge und das Auslesen deren Länge, bei großen Datenmengen zu einem stetig steigenden Speicherplatzbedarf führt, ist eine speicherplatzeffiziente Lösung mit annähernd korrektem Ergebnis vorzuziehen.

Dieses Problem wurde schon in der ersten Version des Algorithmus gelöst und seitdem stetig weiterentwickelt.
So konnte der Speicherbedarf bei der Weiterentwicklung des LogLog- zum HyperLogLog-Algorithmus um 64\% gesenkt werden, ohne dabei Präzision des Ergebnisses einzubüßen \cite{flajolet2007}.

Die Funktionsweise des HyperLogLog-Algorithmus wird in \autoref{sec:proof-of-concept-funktionsweise} detaillierter behandelt und deswegen hier nicht weiter beschrieben.

\subsection{T-Digest}

Der T-Digest ist ein von Ted Dunning entwickelter Algorithmus zum approximieren der Verteilung von Daten mit Quantilen \cite{dunning2019}.

Quantile sind Schwellenwerte auf einer Verteilungsfunktion, die für einen gegebenen Prozentsatz angeben, welche Werte unterhalb dieses Schwellenwerts liegen.
So liefert das 25\%-Quantil beispielsweise den Wert, für den gilt, dass 25\% aller Werte in der zugrundeliegenden Datenmenge kleiner sind als dieser Wert.
Das 50\%-Quantil entspricht somit dem Median einer Datenmenge -- dem Wert, der genau in der Mitte der geordneten Liste steht.

Bei einer naiven Lösung dieser Aufgabe müssen alle eingehenden Daten in einer Liste gespeichert und geordnet werden, woraufhin anschließend die Quantilen ausgelesen werden können.
Beim T-Digest wird die Verteilung der Daten auf wichtige Punkte, in deren Umgebung viel Veränderung stattfindet, heruntergebrochen \cite{davidsonpilon2015}.
Diesen Schwerpunkten (engl. Centroids) wird ein Gewicht zugewiesen, woraus sich später eine gute Approximation der Verteilung ableiten lässt.
Laut Dunning \cite{dunning2015} ist hierbei eine höhere Präzision vor allem in den Randbereichen der Verteilung von besonderer Wichtigkeit.
So sei eine Abweichung von etwa 0,5\% im Bereich des Medians in Ordnung, in Extremfällen wie dem 99,99\%-Quantil jedoch nicht mehr, da es hier darauf ankommt, einen sehr kleinen Bereich zu identifizieren.
Die Verteilung der Gewichte auf Schwerpunkte und das Auslassen von Punkten ist in \autoref{fig:t-digest} dargestellt.
In diesem Beispiel wurden 100\,000 zufällige Zahlen im Bereich von 1 bis 1\,000, deren Verteilung links zu sehen ist, durch den T-Digest Algorithmus auf Schwerpunkte reduziert und mit einem Gewicht versehen.
Hierbei bleiben die Randbereiche größtenteils erhalten und bieten so die höchste Präzision, während zur Mitte hin die Cluster, welche zu einem stärker gewichteten Schwerpunkt zusammengefasst werden, größer werden.

\begin{figure}[b]
	\centering
	\includegraphics[width=.49\linewidth]{images/t_digest_data.png}
	\includegraphics[width=.49\linewidth]{images/t_digest_sorted.png}
	\caption{Schwerpunktverteilung durch T-Digest}
	\label{fig:t-digest}
\end{figure}

In einem selbst durchgeführten Test, in dem eine Implementierung des T-Digest-Algorithmus mit einer einfachen, sortierten Liste verglichen wurde, hat sich die Präzision des T-Digest als sehr gut herausgestellt.
Es wurden 100\,000 verschiedene, zufällige Zahlen von 1 bis 1\,000 in beide Objekte eingetragen, wobei anschließend je der Median, sowie das 99,99\%-Quantil geprüft wurden.

Hierbei ergab sich, dass der Median zwar nur in 0,33\% der Fälle akkurat war, jedoch nie eine Abweichung von $\pm$\,0,5\% überschritt.
Für das 99,99\%-Quantil war der T-Digest in 99,79\% der Fälle akkurat, in den übrigen Fällen wies er eine Abweichung von maximal $\pm$\,0,01\% auf.

Außerdem hat sich die von Dunning und Ertl \cite{dunning2019} beschriebene, sehr hohe Geschwindigkeit bei diesem Test ebenso bewahrheitet.
Sowohl das Einfügen in die Liste, als auch in den T-Digest bewegt sich durchschnittlich in der gleichen Größenordnung von 10\textsuperscript{-5}\,Sekunden (10\,\textmu{}s).
Gleiches gilt für das identifizieren eines Quantils, was in beiden Fällen in der Größenordnung 10\textsuperscript{-4}\,Sekunden (100\,\textmu{}s) passiert.

Ein weiterer Vorteil des T-Digest-Algorithmus ist, dass die Daten mehrerer Digests einfach verbunden werden können, solange man die Gewichte der Schwerpunkte beachtet \cite{dunning2019}.
Dies ist zum Beispiel nützlich, wenn man einen Median aus mehreren, großen Datensätzen identifizieren möchte, da es hierfür nicht einfach reicht, den Median für jeden Datensatz einzeln zu identifizieren und aus der Liste der Ergebnisse einen neuen Median zu ziehen.


\subsection{Count-Min-Sketch}

Der Count-Min-Sketch ist ein Algorithmus von Cormode und Muthukrishnan \cite{cormode2005}, 
mit dem die Häufigkeit aller Elemente in einem Stream approximativ ermittelt werden kann. 
Die Häufigkeiten werden dabei in ein Array geschrieben, 
wodurch die benötigte Speichergröße für die Datenmenge stark komprimiert, 
aber dafür etwas ungenauer wird. 
Der Algorithmus weist für häufige Elemente eine hohe Genauigkeit auf 
und wird deshalb für die Ermittlung der Häufigkeit von Elementen genutzt, 
von denen erwartet wird, dass sie oft in dem Stream enthalten sind.

Um die Häufigkeit eines Elements zu ermitteln 
nutzt der Algorithmus ein Array mit mehreren Zeilen 
und ebenso viele verschiedene Hash-Funktionen.
Der Zusammenhang zwischen Hash-Funktionen und den Zeilen durch die einzelnen Reihen in \autoref{fig:count-min-sketch} dargestellt. 
Jedes Element des Streams wird mit den Hash-Funktionen gehasht 
und in die der Hash-Funktion zugeteilte Zeile eingetragen. 
Wird ein Hash-Wert in einer Zeile mehrfach hinzugefügt -- egal ob durch den selben oder einen anderen Wert -- 
erhöht sich ein Counter für diesen Hash-Wert. 
Diese Erhöhung ist in \autoref{fig:count-min-sketch} durch ein +1 in einer Zelle für den entsprechenden Hash-Wert dargestellt.
Es kann passieren, dass für verschiedene Werte der selbe Hash-Wert erzeugt 
und so auch der gleiche Counter erhöht wird.
Durch dieses Verhalten kann für einen einzelnen Hash-Wert 
schlussendlich nicht mehr unterschieden werden, 
wie hoch die Häufigkeit eines Elements unter diesem Hash-Wert ist. 
Es ergibt sich jedoch, dass der Hash-Wert des Elements, dessen Count am geringsten ist, 
durch die wenigsten Hash-Werte anderer Elemente erhöht wurde. 
Somit ist der Hash-Wert des Elements mit dem geringsten Count der, 
der dem tatsächlichen Wert am nächsten kommt \cite{cormode2017}.

\begin{figure}
	\centering
	\begin{tikzpicture}[
		x=1cm,
		y=1cm,
		hashstyle/.style={draw, minimum height=.75cm, minimum width=.75cm},
		cellstyle/.style={draw, minimum height=.6cm, minimum width=.6cm, font=\footnotesize}
	]
		\node (stream) at (-3, 1) {Stream};
		
		\node[hashstyle] (h1) at (0, 2) {h\textsubscript{1}};
		\node[hashstyle] (h2) at (0, 1) {h\textsubscript{2}};
		\node[hashstyle] (h3) at (0, 0) {h\textsubscript{3}};
		
		\node[cellstyle] (1-1) at (2, 2) {};
		\node[cellstyle] (1-2) at (2.6, 2) {};
		\node[cellstyle] (1-3) at (3.2, 2) {+1};
		\node[cellstyle] (1-4) at (3.8, 2) {};
		\node[cellstyle] (1-5) at (4.4, 2) {};
		
		\node[cellstyle] (2-1) at (2, 1) {+1};
		\node[cellstyle] (2-2) at (2.6, 1) {};
		\node[cellstyle] (2-3) at (3.2, 1) {};
		\node[cellstyle] (2-4) at (3.8, 1) {};
		\node[cellstyle] (2-5) at (4.4, 1) {};
		
		\node[cellstyle] (3-1) at (2, 0) {};
		\node[cellstyle] (3-2) at (2.6, 0) {};
		\node[cellstyle] (3-3) at (3.2, 0) {};
		\node[cellstyle] (3-4) at (3.8, 0) {+1};
		\node[cellstyle] (3-5) at (4.4, 0) {};
		
		\draw[-latex] (stream.east) -- +(1, 0) -- +(1, 1) -- (h1.west);
		\draw[-latex] (stream.east) -- (h2.west);
		\draw[-latex] (stream.east) -- +(1, 0) -- +(1, -1) -- (h3.west);
		
		\draw[-latex] (h1.east) -- (1-3.west) -- +(0.1, 0);
		\draw[-latex] (h2.east) -- (2-1.west) -- +(0.1, 0);
		\draw[-latex] (h3.east) -- (3-4.west) -- +(0.1, 0);
	\end{tikzpicture}
	\caption{Zählvorgang im Array des Count-Min-Sketch}
	\label{fig:count-min-sketch}
\end{figure}

Der Aufbau des Algorithmus ähnelt dem des Bloom Filter Algorithmus \cite{cormode2017}. 
Das Array, das für den Bloom Filter Algorithmus verwendet wird, 
hat jedoch nur eine Zeile in die die gehashten Werte aller Hash-Funktionen eingetragen werden \cite{cormode2017}.

Durch die Erhöhung der Menge verschiedener Hash-Funktionen kann die Abweichung des Counts reduziert werden. 
Somit lässt sich durch Erhöhen oder Reduzieren der Menge an Hash-Funktionen 
der Count-Min-Sketch dem Bedürfnis von Präzision oder Geschwindigkeit anpassen.

\subsection{StreamApprox}

StreamApprox ist ein Sampling Algorithmus der dafür entwickelt wurde, 
die Datenmenge eines Streams auf eine bestimmte Größe zu limitieren, 
indem er nur einige Daten des Streams weiterreicht. 
Dabei erfolgt die Selektierung, welche Daten weitergegeben werden, durch einen Zufallsalgorithmus. 
Auf diese Weise wird das Ergebnis der Auswertung des Streams weniger stark beeinträchtigt, 
als wenn die Daten, die die Verarbeitungskapazitäten überschreiten, 
überhaupt nicht betrachtet werden. 
Der Algorithmus ist dabei sowohl auf Batch-, als auch auf Pipeline-basierte Streams anwendbar.
Das besondere an StreamApprox ist die Fähigkeit, 
die Größe des Streams zu limitieren und dabei so wenig Daten wie möglich aus dem Stream zu entfernen. 
Der Algorithmus reduziert also nur dann den Datenstrom, wenn dessen Datenmenge ein eingestelltes Limit überschreitet. 
Diese Fähigkeit übersteigt die von Random Sampling Algorithmen, 
die für eine ähnliche Ausgabe die absolute Anzahl an Elementen des Streams kennen müssten. 
Ermöglicht wird das durch die Verwendung von sog. Reservoir Sampling, 
bei dem die Elemente des Streams in einen Pufferspeicher geschrieben werden, 
wobei neue Elemente an einer zufälligen Stelle des Puffers den vorherigen Wert überschreiben, sobald dieser voll ist \cite{quoc2017}. 

Dieses Vorgehen kann hilfreich sein, 
wenn die Datenmenge für die gewünschte Analyse stark variiert oder zu groß ist und reduziert werden soll, 
um eine bestimmte Verarbeitungsgeschwindigkeit beizubehalten, 
ohne das Ergebnis der Analyse zu stark zu beeinflussen.
Bei einer Entfernung der Daten mit einem Muster, zum Beispiel Überschreiben jedes zweiten Felds im Pufferspeicher, 
könnten so eventuelle Muster einkommender Daten stärker betroffen sein, als bei einem zufälligen Überschreiben.
Es kann ebenfalls hilfreich sein, 
wenn, aufgrund fehlender Ressourcen zur Verarbeitung, 
die Menge an gelieferten Daten reduziert werden soll.